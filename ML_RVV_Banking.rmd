---
title: "ML algorithm for Harvardx Capstone Course, Choose Your Own project"
author: "Rubén Vázquez del Valle"
date: "2/10/2021"
output:
  html_document:
    df_print: kable
    fig_caption: true
    toc: true
    latex_engine: xelatex

---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE)
```



# 1.Introduction


According to wikipedia:


    1. Marketing is currently defined by the American Marketing Association (AMA) as " the performance of business activities that direct the flow of goods, and services from producers to consumers"
    2. Direct marketing is a form of communicating an offer, where organizations communicate directly to a pre-selected customer and supply a method for a direct response.



Hence we can understand that direct marketing campaigns are several processes producers undertake to engage directly its target consumers, build strong relationships to create value in order to capture value in return and get a fast and direct response.

As time goes by and technology advances those processes evolved, and keep on evolving, from different analog channels such as reply cards, reply forms to be sent in an envelope, to new and more sophisticated digital ones such as  websites, text messages sent to cellular phone and/or email addresses.

One not so long old-fashioned direct marketing technique was phone calls.

The purpose of this project is analyzing if such technique applied in this case by a Portuguese banking institution not so much time ago, had positive effect or not in its clients, or in other words building a classification system to predict if the client would subscribe a term deposit.

The origin dataset has been downloaded from The UCI Machine Learning Repository: 
https://archive.ics.uci.edu/ml/machine-learning-databases/00222/


Once dataset was created, I have based my work on *"HarvardX - PH125.8x course: Data Science - Machine Learning"* specifically on machine learning techniques applied to supervised learning.



```{r loading-libs, message=FALSE}

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(rvest)) install.packages("rvest", repos = "http://cran.us.r-project.org")
if(!require(httr)) install.packages("httr", repos = "http://cran.us.r-project.org")
if(!require(genefilter)) install.packages("genefilter", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(rvest)
library(httr)
library(genefilter)

```


```{r loading-functions, message=FALSE}
###############################################################################
###############################################################################
##        Functions specifically created for the project
###############################################################################
###############################################################################

## substrRight function to substract n last characters on a string
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}


## substrLeft function to substract n first characters on a string
substrLeft <- function(x, n){
  substr(x, 1, n)
}


```

\newpage  

# 2 Methodology

Next step consist on analyzing the data provided, cleaning, wrangling and preparing it in case of actions were needed to decide which kind of algorithms will be worthy in terms of classification problems.


## 2.1 Exploratory Analysis


```{r Downloading-data, message=FALSE}

################################################################################################################
# Download source data set, and split it between training, test and validation sets (final hold-out test set)
################################################################################################################

# Note: this process could take a couple of minutes


# URL with original dataset to be downloaded
url<-"https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#"
url2<-"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/"
  
  
file<-"bank-additional.zip"

dfile<-paste(url2,file,sep="")


# Create input folder on working directory to download source files
mydir<-getwd()
inputdir<-paste(mydir,"/Input",sep="")
dir.create(path=inputdir,mode="0777")

# Download source files, unzip them and erase temp files
dl <- tempfile()
download.file(dfile,dl)
untar(dl,exdir=inputdir)
rm(dl)


#Looking for downloading directory
dirs<-list.dirs()
idx_dirs<-grep(paste("/Input",substrLeft(file,15),sep="/"),dirs)
dw_dir<-dirs[idx_dirs]

#List files tp download
dw_files<-list.files(path=dw_dir)

#Download n all files and save them into Input_file_n  tibbles
for(i in (1:length(dw_files))){
exp1 <- expression(paste("Input_file",as.character(i),sep="_"))                                  # Create name of the tibble expression
exp2<- expression(read_delim(file=paste(dw_dir,dw_files[i],sep="/"),delim=";",col_names=TRUE))   # Create read_delim expression
z<-paste(eval(exp1),exp2,sep="<-")                                                               # Create assignation expression as a string
eval(parse(text=z))                                                                              # Evaluate expression
}

```

After downloading source data it is observable that there are three files compressed: `r format(dw_files,big.mark=",")`.


`r format(dw_files[2],big.mark=",")` is a text document with some basic information about the remaining ones. Here it is stated that `r format(dw_files[3],big.mark=",")` is a random sample subset of `r format(dw_files[1],big.mark=",")` 

Applying basic relational algebra, a simple way to confirm that a set B is a subset of another one A, is getting the size of the anti_join function. While anti_join() return all rows from B without a match in A, having and antijoin by all columns with size 0  means that all rows in B are present in A, which indeed confirms that B is a subset of A.

Hence applying anti_join to  `r format(dw_files[3],big.mark=",")` on `r format(dw_files[1],big.mark=",")` by all columns, the number of rows obtained is: `r format(nrow(anti_join(Input_file_3,Input_file_1,by=NULL)),big.mark=",",scientific=F)` which confirms that data file to be used is `r format(dw_files[1],big.mark=",")`


Now that source data has been properly addressed, let's continue by exploring and summarizing the dataset `r format(dw_files[1],big.mark=",")`:


```{r Exploratory Analysis, message=FALSE}

head(Input_file_1[1:10])
head(Input_file_1[11:21])
summary(Input_file_1)


#Build an index with all character columns in the dataset that will need to become factor
idx_isc<-which(sapply(Input_file_1,is.character)==TRUE)

```

At simple glance, it can be observed that the dataset is a data.frame containing `r format(nrow(Input_file_1),big.mark=",",scientific=F)` rows and `r format(ncol(Input_file_1),big.mark=",",scientific=F)` columns, the last one with the prediction values. It is also evident the split between numerical data and categorical data. In order to be able to address this first issue it is necessary to transform characters into factors for columns: `r format(colnames(Input_file_1[idx_isc]),big.mark=",",scientific=F)` 

Let's explore possible values for the categorical columns 


```{r Exploratory Analysis 2, message=FALSE}
# Get values for character columns
for(i in (1:length(idx_isc))){
  #The idea is building for each character column the sentence that will cast it to a factor column. For instance:
  # Input_file_1$job<-factor(Input_file_1$job)
  # Instead of building one by one and taking advantage of having them stored in idx_isc, I will build the expression.
  exp1 <- expression(paste("Input_file_1",colnames(Input_file_1[idx_isc])[i],sep="$"))       # Input_file_1$**** expression
  exp2<-expression(factor(eval(parse(text=eval(exp1)))))                                     # factor(Input_file_1$****) expression
  exp3<-expression(levels(factor(eval(parse(text=eval(exp1))))))                             # levels(factor(Input_file_1$****)) expression
  cat(colnames(Input_file_1[idx_isc])[i],"possible values are :",eval(exp3),"\n")
  z<-paste(eval(exp1),exp2,sep="<-")                                                         # Create a<-factor(a) expression 
  eval(parse(text=z))    
}  

```



Once categorical values have been transformed from character to factor, let's look again how the values of those columns are distributed
```{r Exploratory Analysis 3, message=FALSE}
summary(Input_file_1[idx_isc])


```


At this point, it is fair to assume then that description of columns of our dataset provided in file `r format(dw_files[2],big.mark=",")` is correct, stating the following:


   Input variables:
   # bank client data:
   1 - age (numeric)
   2 - job : type of job (categorical: "admin.","blue-collar","entrepreneur","housemaid","management","retired","self-employed","services","student","technician","unemployed","unknown")
   3 - marital : marital status (categorical: "divorced","married","single","unknown"; note: "divorced" means divorced or widowed)
   4 - education (categorical: "basic.4y","basic.6y","basic.9y","high.school","illiterate","professional.course","university.degree","unknown")
   5 - default: has credit in default? (categorical: "no","yes","unknown")
   6 - housing: has housing loan? (categorical: "no","yes","unknown")
   7 - loan: has personal loan? (categorical: "no","yes","unknown")
   # related with the last contact of the current campaign:
   8 - contact: contact communication type (categorical: "cellular","telephone") 
   9 - month: last contact month of year (categorical: "jan", "feb", "mar", ..., "nov", "dec")
  10 - day_of_week: last contact day of the week (categorical: "mon","tue","wed","thu","fri")
  11 - duration: last contact duration, in seconds (numeric). Important note:  this attribute highly affects the output target (e.g., if duration=0 then y="no"). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.
   # other attributes:
  12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
  13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
  14 - previous: number of contacts performed before this campaign and for this client (numeric)
  15 - poutcome: outcome of the previous marketing campaign (categorical: "failure","nonexistent","success")
   # social and economic context attributes
  16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
  17 - cons.price.idx: consumer price index - monthly indicator (numeric)     
  18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)     
  19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
  20 - nr.employed: number of employees - quarterly indicator (numeric)

  Output variable (desired target):
  21 - y - has the client subscribed a term deposit? (binary: "yes","no")



## 2.2.Transforming and partitioning original dataset


## 2.3 Modelling


### 2.3.1 BASELINE PREDICTORS


\newpage

# 3.Results


\newpage

# 4.Conclusions


\newpage

# 5.Bibliography
