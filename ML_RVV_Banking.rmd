---
title: "ML algorithm for Harvardx Capstone Course, Choose Your Own project"
author: "Rubén Vázquez del Valle"
date: "4/12/2021"
bibliography: bibliography.bib
biblio-style: apalike
link-citations: TRUE
urlcolor: blue
output:
  pdf_document:
    df_print: kable
    fig_caption: true
    toc: true
    latex_engine: xelatex
    citation_package: natbib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE)
```

\newpage  

# 1.Introduction

According to [Wikipedia:](https://en.wikipedia.org/wiki/Marketing)
 

> 1.  Marketing is currently defined by the American Marketing Association (AMA) as " the performance of business activities that direct the flow of goods, and services from producers to consumers"
> 2.  Direct marketing is a form of communicating an offer, where organizations communicate directly to a pre-selected customer and supply a method for a direct response.



Hence we can understand that direct marketing campaigns are several processes producers undertake to engage directly its target consumers, build strong relationships to create value in order to capture value in return and get a fast and direct response.

As time goes by and technology advances those processes evolved, and keep on evolving, from different analog channels such as reply cards, reply forms to be sent in an envelope, to new and more sophisticated digital ones such as  websites, text messages sent to cellular phone and/or email addresses.

One not so long old-fashioned direct marketing technique was phone calls.

The purpose of this project is analyzing if such technique applied in this case by a Portuguese banking institution not so much time ago, had positive effect or not in its clients, or in other words building a classification system to predict if the client would subscribe a term deposit.

The origin dataset has been downloaded from [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/)


Once dataset was created, I have based my work on [@rafalab] *"HarvardX - PH125.8x course: Data Science - Machine Learning"* specifically on machine learning techniques applied to supervised learning.



```{r loading-libs, message=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(rvest)) install.packages("rvest", repos = "http://cran.us.r-project.org")
if(!require(httr)) install.packages("httr", repos = "http://cran.us.r-project.org")
if(!require(ggcorrplot)) install.packages("ggcorrplot", repos = "http://cran.us.r-project.org")
if(!require(fastDummies)) install.packages("fastDummies", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(rattle)) install.packages("rattle", repos = "http://cran.us.r-project.org")


library(tidyverse)
library(caret)
library(data.table)
library(rvest)
library(httr)
library(ggcorrplot)
library(fastDummies)
library(e1071)
library(rpart)
library(randomForest)
library(rattle)

```


```{r loading-functions, message=FALSE}
###############################################################################
###############################################################################
##        Functions specifically created for the project
###############################################################################
###############################################################################

## substrRight function to substract n last characters on a string
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}


## substrLeft function to substract n first characters on a string
substrLeft <- function(x, n){
  substr(x, 1, n)
}


```

\newpage  

# 2 Methodology

On this section, following tasks such as  analyzing the data provided, cleaning, wrangling and preparing it in case of actions were needed to decide which kind of algorithms will be worthy in terms of classification problems, will be adressed.

Thus next steps will be:

  A preliminary analysis where basically observing and sumarizing source files and datasets.
  
  A cross validation to split source dataset into training, testing and validation datasets. 

  A bivariate and multivariate analysis on training dataset to decide which features include or discard from candidate models. 

  A modeling phase where some classifier models will be trained to predict dependent variable. 
  
  A decision phase where based on results obtained on test set with the previous models deciding which one to choose or ensemble.


## 2.1 Preliminary Analysis


```{r Downloading-data, message=FALSE}

################################################################################################################
# Download source data set, and split it between training, test and validation sets (final hold-out test set)
################################################################################################################

# Note: this process could take a couple of minutes


# URL with original dataset to be downloaded
url<-"https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#"
url2<-"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/"
  
  
file<-"bank-additional.zip"

dfile<-paste(url2,file,sep="")


# Create input folder on working directory to download source files
mydir<-getwd()
inputdir<-paste(mydir,"/Input",sep="")
dir.create(path=inputdir,mode="0777")

# Download source files, unzip them and erase temp files
dl <- tempfile()
download.file(dfile,dl)
untar(dl,exdir=inputdir)
rm(dl)


#Looking for downloading directory
dirs<-list.dirs()
idx_dirs<-grep(paste("/Input",substrLeft(file,15),sep="/"),dirs)
dw_dir<-dirs[idx_dirs]

#List files tp download
dw_files<-list.files(path=dw_dir)

#Download n all files and save them into Input_file_n  tibbles
for(i in (1:length(dw_files))){
exp1 <- expression(paste("Input_file",as.character(i),sep="_"))                                  # Create name of the tibble expression
exp2<- expression(read_delim(file=paste(dw_dir,dw_files[i],sep="/"),delim=";",col_names=TRUE))   # Create read_delim expression
z<-paste(eval(exp1),exp2,sep="<-")                                                               # Create assignation expression as a string
eval(parse(text=z))                                                                              # Evaluate expression
}

```

After downloading source data it is observable that there are three files compressed: `r format(dw_files,big.mark=",")`


`r format(dw_files[2],big.mark=",")` is a text document with some basic information about the remaining ones. Here it is stated that `r format(dw_files[3],big.mark=",")` is a random sample subset of `r format(dw_files[1],big.mark=",")`.

Applying basic relational algebra, a simple way to confirm that a set B is a subset of another one A, is getting the cardinality of B/A, or equivalently getting the size of the anti_join function. While anti_join() return all rows from B without a match in A, having and antijoin by all columns with size 0  means that all rows in B are present in A, which indeed confirms that B is a subset of A.

Hence applying anti_join to  `r format(dw_files[3],big.mark=",")` on `r format(dw_files[1],big.mark=",")` by all columns, the number of rows obtained is: `r format(nrow(anti_join(Input_file_3,Input_file_1,by=NULL)),big.mark=",",scientific=F)` which confirms that data file to be used is `r format(dw_files[1],big.mark=",")`

 Besides, `r format(dw_files[2],big.mark=",")` also provides us with a description of the fields in `r format(dw_files[1],big.mark=",")` as follows:

    1.  age: Age of the client.
    2.  job: Type of job of the client.
    3.  marital: Marital status, note "divorced" means divorced or widowed.
    4.  education: Educational degree reached by the client. 
    5.  default: Binary variable meaning if the client has credit/credits in default.
    6.  housing: Binary variable meaning if the client has a mortgage.
    7.  loan: Binary variable meaning if the client has a personal loan.
    8.  contact: Contact communication type with the client.
    9.  month: Last contact month of year.
    10. day_of_week: Last contact day of the week.
    11. duration: Last contact duration, in seconds. 
    12. campaign: For each  client, number of contacts performed during this campaign. 
    13. pdays: Days since last contact from a previous campaign (999 means not previously contacted).
    14. previous: For each  client, number of contacts performed before this campaign.
    15. poutcome: Outcome of the previous marketing campaign if existing.
    16. emp.var.rate: Employment variation rate - quarterly indicator.
    17. cons.price.idx: Consumer price index - monthly indicator.
    18. cons.conf.idx: Consumer confidence index - monthly indicator.
    19. euribor3m: Euribor 3 month rate - daily indicator.
    20. nr.employed: Number of employees - quarterly indicator.
    21. y: Output binary variable if the client has subscribed a term deposit or not. 
    
Now that source data has been properly addressed, let's continue by exploring and summarizing the dataset `r format(dw_files[1],big.mark=",")`:


```{r Exploratory Analysis, message=FALSE}

head(Input_file_1[1:8])
head(Input_file_1[9:15])
head(Input_file_1[16:21])
summary(Input_file_1)



#Build an index with all character columns in the dataset that will need to become factor
idx_isc<-which(sapply(Input_file_1,is.character)==TRUE)

```

At simple glance, it can be observed that the dataset is a data.frame containing `r format(nrow(Input_file_1),big.mark=",",scientific=F)` rows and `r format(ncol(Input_file_1),big.mark=",",scientific=F)` columns, the last one with the prediction values. It is also evident the split between numerical data and categorical data. In order to be able to address this first issue it is necessary to transform characters into factors for columns: `r format(colnames(Input_file_1[idx_isc]),big.mark=",",scientific=F)` 


Let's explore possible values for the categorical columns: 

```{r Exploratory Analysis 2, message=FALSE}
# Get values for character columns
for(i in (1:length(idx_isc))){
  #The idea is building for each character column the sentence that will cast it to a factor column. For instance:
  # Input_file_1$job<-factor(Input_file_1$job)
  # Instead of building one by one and taking advantage of having them stored in idx_isc, I will build the expression.
  exp1 <- expression(paste("Input_file_1",colnames(Input_file_1[idx_isc])[i],sep="$"))       # Input_file_1$**** expression
  exp2<-expression(factor(eval(parse(text=eval(exp1)))))                                     # factor(Input_file_1$****) expression
  exp3<-expression(levels(factor(eval(parse(text=eval(exp1))))))                             # levels(factor(Input_file_1$****)) expression
  cat(colnames(Input_file_1[idx_isc])[i],"possible values are :","\n")
  print(eval(exp3))
  cat("_______________________________________________________________________________","\n")
  z<-paste(eval(exp1),exp2,sep="<-")                                                         # Create a<-factor(a) expression 
  eval(parse(text=z))    
}  
```
There is also a second fact observable at simple glance. There seems not to be any missing values within our data, since whenever it occurred the field was populated with "unknown".


At this point, it is fair to assume then that description of columns of our dataset provided in file `r format(dw_files[2],big.mark=",")` is correct. It is also important to recall the prevalence effect on binary variable to be predicted, around  `r format(100*mean(Input_file_1$y=="no"),big.mark=",")`% of the answers are no, which a priori matches what I would expect in terms of this marketing campaign success ratio.

## 2.2.Partitioning, preprocessing and transforming original dataset

As soon as original dataset inspection is concluded next step consists on partitioning, preprocessing and transforming the source dataset. 

Let's start by partitioning it. In order to avoid as much as possible over-training I will apply K-fold cross validation. As I do not have an independent dataset where validate the results of my models, I will remove randomly a part of the original dataset splitting it in two sets: the training dataset that I will refer as bankraw and the validation dataset, that I will call validation.


To do this split I applied Pareto principle, choosing 80% of original dataset as training dataset and remaining 20% as validation one. Validation dataset will only and exclusively be used for evaluation purposes at the end of the project.



```{r Partitioning 1, message=FALSE}

# Validation set will be 20% of Source data according to Pareto principle
set.seed(1978, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = Input_file_1$y, times = 1, p = 0.2, list = FALSE)
bankraw<- Input_file_1[-test_index,]
Validation <- Input_file_1[test_index,]


```


Once categorical values have been transformed from character to factor, and partitions between bankraw and validation have been created, let's look again how the values of those columns are distributed within bankraw dataset:
```{r Partitioning 2, message=FALSE}

summary(bankraw)


```

It is remarkable that at simple visual inspection this split of original dataset does not differ much from the original one.

Finally, now that bankraw and validation datasets were created, I split bankraw in train_set and test_set applying once again Pareto's law, and I will use theses train_set and test_set to train different models and test their performance over different metrics prior to decide which candidate model will be the final one. Once final model is chosen, and only at that point, validation dataset will be used. 

```{r Partitioning 3, message=FALSE}


## Selecting a random seed to allow replicability
set.seed(1978, sample.kind="Rounding")


## Creating training a testing partitions on bankraw dataset 

test_index <- createDataPartition(y = bankraw$y, times = 1, p = 0.2, 
                                  list = FALSE)
train_set <- bankraw[-test_index,]
test_set <- bankraw[test_index,]

```


## 2.3.Exploratory Analysis

At this point it is important to visualize the data we have, so I will plot first of all numeric variables and second of all factor variables within the training set.

It is important to note that a very simple and common form of statistical analysis is the bivariate analysis. In consists on comparing two variables, one against the other. In our case, it is a matter of comparing each independent variable against the dependent variable "Y" we want to predict.

Let's start with the histograms for numeric variables splitting by dependent variable. This will allow us to get a first impression on those numeric variables distributions and their influence on the final result. Figures 1 to 10 will show this  analysis:

```{r Exp_Analysis, message=FALSE}
colnames_num<-colnames(train_set[-idx_isc])

for(i in (1:length(colnames_num))){
  print(    
    train_set%>%ggplot(aes(eval(parse(text=eval(colnames_num[i]))))) +
    geom_histogram() +
    facet_wrap(~y) +
    ggtitle(paste("Bivariate Analysis on variable",colnames_num[i])) +
    xlab(colnames_num[i]) +
    labs(caption = paste("Figure ",i))
  )
  }
```

Now bivariate analysis has been performed on numeric independent variables, let's also apply it to discrete independent variables.


Figures 11 to 20 will show the bivariate analysis on categorical variables.

```{r Exp_Analysis 2, message=FALSE}

colnames_fac<-colnames(train_set[idx_isc])

n<-length(colnames_fac)-1

for(i in (1:n)){
  print(    
    train_set%>%ggplot(aes(y)) +
    geom_bar() +
    facet_wrap(~eval(parse(text=eval(colnames_fac[i])))) +  
        ggtitle(paste("Bivariate Analysis on variable",colnames_fac[i])) +
    labs(caption = paste("Figure ",10+i))
  )
}


```

Bivariate analysis is a simple case of multivariate analysis, instead of comparing variables in pairs, multiple variables are compared against each other at the same time. This may allow us to identify prediction power, which variables are providing same kind of information and finally allow us to choose if all variables make sense within the model or on the other hand some of them can de disregarded.

In order to perform this multivariate analysis, let's analyze Pearson correlation between numeric variables and dependent variable "Y"


```{r Exp_Analysis 3, message=FALSE}
Y<-as.numeric(train_set$y)
M<-cbind(train_set[-idx_isc],Y)
MC<-cor(M)
ggcorrplot(MC)+
  ggtitle("Pearson's Correlation between numerical variables")+
  labs(caption = "Figure 21")
```

Now, let's do the same for categorical variables, although here some pieces of extra work will be needed.
Where a categorical variable has more than two categories, it can be represented by a set of binary variables, with one variable for each category. In R, there are several ways to perform this task, being one of them the package fastDummies. Function dummy_cols on this package will allow to "binarize" discrete variables. 

Once discrete variables have been "binarized", let's perform also Pearson's correlation. For simplicity's sake, due to high number of dummy variables created, instead of plotting all together, I have plotted them in two halves.


```{r Exp_Analysis 4, message=FALSE}
new_train_set<-train_set%>%select(colnames_fac) %>% 
               dummy_cols(., select_columns = c("job","marital","education","contact","month","poutcome"))

MC1<-cbind(new_train_set[,12:35],Y)
MC2<-cbind(new_train_set[,36:50],Y)

MCC1<-cor(MC1)
ggcorrplot(MCC1)+
  ggtitle("Pearson's Correlation between categorical variables")+
  labs(caption = "Figure 23")



MCC2<-cor(MC2)
ggcorrplot(MCC2)+
  ggtitle("Pearson's Correlation between categorical variables")+
  labs(caption = "Figure 24")


```


At this point some relevant information has been unveiled.Some variables seem to be more relevant regarding "Y" than others.
Hence following variables will be the candidate ones to take part in my models: 


```{r Exp_Analysis 5, message=FALSE}


## Selecting principal variables, correlation greater than an arbitrary threshold

idx1<-which(abs(MC[,11])>=0.3)

idx2<-which(abs(MCC1[,25])>=0.3)

idx3<-which(abs(MCC2[,16])>=0.3)

## Taking off dependent variable from independent variables list

fidx1<-grepl("Y",names(idx1))

fidx2<-grepl("Y",names(idx2))

fidx3<-grepl("Y",names(idx3))

## Storing principal variables into final_vars vector


final_vars<-c(names(idx1[!fidx1]),names(idx2[!fidx2]),substrLeft(names(idx3[!fidx3]),8))

final_vars

reduced_train_set<-train_set%>%select(final_vars,y)

```


## 2.4 Modelling

As previously stated the final objective is building a classification system to predict if the client would subscribe a term deposit or not. Supervised learning classification algorithms learn from labeled data, in our case the bankraw dataset we have already described, with the "y" label being a binary variable, so we are facing the most simple classification problem.

Nowadays there are several well-known supervised learning classification algorithms such as logistic regression, k nearest neighbors, decision trees, random forest, support vector machines or neuronal networks, having all of them different pros and cons.

On their paper [@Moro], they compared logistic regression, decision trees, support vector machines and neuronal networks finding that neuronal networks performed better than others. In this project, due to computational resources needed for running support vector machines and neuronal networks, I have discarded them focusing only on logistic regression, k nearest neighbors, decision trees and  random forest.

Besides, following approach taken in my previous project [ML_RVV_Capstone_Project](https://github.com/rvazquezv/Capstone) I have also included a baseline predictor to set as reference.

Finally, some decisions regarding primary metrics used to choose candidates models will also be necessary and discussed later.


### 2.4.1 BASELINE PREDICTORS

Baselines for predictive models or baseline predictors are performance evaluations for given problems in statistics. Baselines are commonly used as the first approach and the limit that should at least be reached.

In doing so, the first baseline stage will be the naive one, just only assuming results will always be no.


$$\hat{Y}=0$$  


being $\hat{Y}$ the predicted result, with 0 as no suscription and 1 as suscription.


```{r Baseline , message=FALSE}
predict_baseline<- sample(c("no"), length(test_set$y), replace = TRUE) %>%
  factor(levels = levels(test_set$y))

Acc<-confusionMatrix(predict_baseline,test_set$y)$overall["Accuracy"]
Bacc<-confusionMatrix(predict_baseline,test_set$y)$byClass["Balanced Accuracy"]
Sen<-confusionMatrix(predict_baseline,test_set$y)$byClass["Sensitivity"]
Spe<-confusionMatrix(predict_baseline,test_set$y)$byClass["Specificity"]
```

As already mentioned earlier, `r format(100*mean(train_set$y=="no"),big.mark=",")`% of answers are no in train set which explains why naive model has `r format(Acc,big.mark=",")` accuracy. Hence prevalency is an issue within our original dataset and accuracy is not the best metric to consider. In order to mitigate prevalence as much as possible  I will also explore balance accuracy, sensitivity and specificity for all candidate models.


In doing so following results are obtained:

```{r Baseline_2 , message=FALSE}
## Save results to table
Results <- tibble(method = "Naive", Accuracy = Acc, Balanced_Accuracy = Bacc, Sensitivity = Sen, Specificity = Spe)
Results

```

As observed in table above accuracy and sensitivity are reasonably good but balanced accuracy and specificity are not good at all as it can be expected due to non uniform distribution in our datasets. 

### 2.4.2 LOGISTIC REGRESSION

Logistic regression  is a specific case of a set of generalized linear models (glm) and it extends the regression approach to categorical data. It measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function.
The logistic regression can be understood simply as finding the $\beta$  parameters that best fit:

$$ \hat{Y}={\begin{cases}1&\beta _{0}+\beta _{1}x+\varepsilon >0\\0&{\text{else}}\end{cases}}$$


Applied to our data using train function in caret package with glm method, following results are obtained:



```{r Logistic_regression, message=FALSE}

train_glm<-train(y~.,method="glm",data=reduced_train_set)
predict_glm<-predict(train_glm,test_set)


Acc_glm<-confusionMatrix(predict_glm,test_set$y)$overall["Accuracy"]
Bacc_glm<-confusionMatrix(predict_glm,test_set$y)$byClass["Balanced Accuracy"]
Sen_glm<-confusionMatrix(predict_glm,test_set$y)$byClass["Sensitivity"]
Spe_glm<-confusionMatrix(predict_glm,test_set$y)$byClass["Specificity"]

## Save results to table
Results<-rbind(Results,tibble(method = "Logistic Regression", Accuracy = Acc_glm, Balanced_Accuracy = Bacc_glm,
                              Sensitivity = Sen_glm, Specificity = Spe_glm))
Results


```

Despite not improving to much the accuracy and worsening the sensitivity, as expected, specificity, without being good enough yet, has improved dramatically and  balanced accuracy also improved `r format((Bacc_glm-Bacc)/Bacc,big.mark=",",scientific=F)` % 

At this point, as expected, it is obvious that logistic regression is a better choice than naive one.


### 2.4.3 K NEAREST NEIGHBOURS

Main idea behind k nearest neighbors, as in all machine learning algorithms, will be using the five features already selected  to estimate the conditional probability function:

$$p(x_{1},x_{2},x_{3},x_{4},x_{5}) = Pr(Y=1 ∣ X_{1} =x_{1} , X_{2} =x_{2}, X_{3} =x_{3}, X_{4} =x_{4}, X_{5} =x_{5} )$$

First it is defined the distance between all observations based on the features. Then, for any point $(x_{1},x_{2},x_{3},x_{4},x_{5})$ for which we want an estimate of $p(x_{1},x_{2},x_{3},x_{4},x_{5})$, we look for the k nearest points to it and then take an average of the 0s and 1s associated with these points. The set of points used to compute this average is called the neighborhood. Hence we are able to obtain $\hat{p}(x_{1},x_{2},x_{3},x_{4},x_{5})$. In order to control the flexibility of the estimation, the only hyperparameter to fine tune in this algorithm is *k*

Previous steps can be done automatically in R using train function in caret package with knn method. Hyperparameter *k* is controlled with tuneGrid function and through trainControl function, cross validation can be controlled also.

Hence, using 10-fold cross validation, fitting 10 versions of kNN to 9 bootstrapped samples, following results are obtained:


```{r k_nearest_neighbours, message=FALSE}
control <- trainControl(method = "cv", number = 10, p = .9)
grid = data.frame(k = seq(9, 72, 9))
train_knn<-train(y~.,method="knn",data=reduced_train_set,tuneGrid = grid,trControl = control)
predict_knn<-predict(train_knn,test_set)

Acc_knn<-confusionMatrix(predict_knn,test_set$y)$overall["Accuracy"]
Bacc_knn<-confusionMatrix(predict_knn,test_set$y)$byClass["Balanced Accuracy"]
Sen_knn<-confusionMatrix(predict_knn,test_set$y)$byClass["Sensitivity"]
Spe_knn<-confusionMatrix(predict_knn,test_set$y)$byClass["Specificity"]

## Save results to table
Results<-rbind(Results,tibble(method = "K Nearest Neighbors", Accuracy = Acc_knn, Balanced_Accuracy = Bacc_knn,
                              Sensitivity = Sen_knn, Specificity = Spe_knn))
Results

```

Results trend observed in logistic regression model keeps also for K nearest neighbor model, thus worsening sensitivity, but improving  accuracy, balanced accuracy and specificity.

### 2.4.4 DECISION TREES


A decision tree is a flowchart-like structure in which each internal node represents a "test" on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.

Hence decision trees clasifiers can be described as the combination of mathematical and computational techniques to categorize a given set of data.

In our case data appears as follows:

$$(x_{1},x_{2},x_{3},x_{4},x_{5},Y)$$
where Y is the dependent variable and features $(x_{1},x_{2},x_{3},x_{4},x_{5})$ already mentioned in previous models the ones to be partitioned to classify the dependent variable.

Due to their simplicity, and the fact of showing powerful insights via visual inspection, decision trees are among the most popular machine learning algorithms.

In R, decision trees can be managed via rpart function, included in rpart package, being complexity parameter (cp), minsplit and minbucket the hyperparameters to fine tune.

In this case I have tested 25 values ranging from 0 to 0,05 for complexity parameter value and left default minsplit value (at least 20) and default minbucket value obtaining following results:

```{r decision_trees, message=FALSE}
train_rpart<-train(y~.,method="rpart",tuneGrid=data.frame(cp=seq(0,0.05,len=25)),data=reduced_train_set)
predict_rpart<-predict(train_rpart,test_set)

Acc_rpart<-confusionMatrix(predict_rpart,test_set$y)$overall["Accuracy"]
Bacc_rpart<-confusionMatrix(predict_rpart,test_set$y)$byClass["Balanced Accuracy"]
Sen_rpart<-confusionMatrix(predict_rpart,test_set$y)$byClass["Sensitivity"]
Spe_rpart<-confusionMatrix(predict_rpart,test_set$y)$byClass["Specificity"]

## Save results to table
Results<-rbind(Results,tibble(method = "Decision Trees", Accuracy = Acc_rpart, Balanced_Accuracy = Bacc_rpart,
                              Sensitivity = Sen_rpart, Specificity = Spe_rpart))
Results

```
As observed once more time balanced accuracy and specificity improves, but this time neither sensitivity nor accuracy worsen, so this is our first model overperforming remaining ones.


### 2.4.5 RANDOM FOREST

Finally, let's try random forest. Random forest are ensemble of trees. The idea is selecting many predictors, by choosing  different decision trees randomly picked over different features each time, and afterwards ensembling those predictors to build the final one. In our case for every observation in the test set, form a prediction $\hat{Y_{j}}$ using a different tree $T_{j}$ with j $\epsilon$ (1,B) each one of the random trees and finally taking most frequent class among  $\hat{T_{1}}$ , ..., $\hat{T_{B}}$

Random forest in R can be handled in several ways, one of them, still on caret package with rf method.
To control the number of variables randomly sampled as candidates at each split we can use mtry, to control the minimum size of terminal nodes we can use nodesize.

I have chosen mtry value as 2 and looked for the best nodesize among 11,21 and 31 obtaining following results:

```{r random_forest, message=FALSE}
nodesize<-seq(11,31,10)
acc<-sapply(nodesize,function(ns){
train(y~.,method="rf",tuneGrid=data.frame(mtry=2),nodesize=ns,data=reduced_train_set)$results$Kappa
})

train_rf<-train(y~.,method="rf",tuneGrid=data.frame(mtry=2),nodesize=nodesize[which.max(acc)],data=reduced_train_set)
predict_rf<-predict(train_rf,test_set)

Acc_rf<-confusionMatrix(predict_rf,test_set$y)$overall["Accuracy"]
Bacc_rf<-confusionMatrix(predict_rf,test_set$y)$byClass["Balanced Accuracy"]
Sen_rf<-confusionMatrix(predict_rf,test_set$y)$byClass["Sensitivity"]
Spe_rf<-confusionMatrix(predict_rf,test_set$y)$byClass["Specificity"]

## Save results to table
Results<-rbind(Results,tibble(method = "Random Forest", Accuracy = Acc_rf, Balanced_Accuracy = Bacc_rf,
                              Sensitivity = Sen_rf, Specificity = Spe_rf))
Results



```

Computational time took longer than decision trees and hyperparameters are not perfectly fine tuned as it can be observed from results above since all metrics but sensitivity have worsen.
 


\newpage

# 3.Results

After comparing results obtained by baseline model, logistic regression, k nearest neighbors, decision trees and  random forest, decision trees are the most promising ones. Intuitively, I could expect better results from random forest but computational time taken did not help while trying to optimize hyperparameters, so this could explain the poorer performance.

For this reason and due to visual interpretation is easily achievable by human eye as it can be appreciated below in Figure 24, I have chosen decision trees as my final model.


```{r Results , message=FALSE}
## Plotting final model
fancyRpartPlot(train_rpart$finalModel,main="Final model results",yesno=2,split.col="black",nn.col="black", 
               caption="Figure 24",palette="Set2",branch.col="black",type=2) 
```

Applying this model to validation dataset following results are obtained:


```{r Results_2 , message=FALSE}
## Applying final model to Validation dataset

final_predict_rpart<-predict(train_rpart,Validation)

Acc_final<-confusionMatrix(final_predict_rpart,Validation$y)$overall["Accuracy"]
Bacc_final<-confusionMatrix(final_predict_rpart,Validation$y)$byClass["Balanced Accuracy"]
Sen_final<-confusionMatrix(final_predict_rpart,Validation$y)$byClass["Sensitivity"]
Spe_final<-confusionMatrix(final_predict_rpart,Validation$y)$byClass["Specificity"]


## Save results to final table
Final_Results <- tibble(method = "Decision Trees", Accuracy = Acc_final, Balanced_Accuracy = Bacc_final, 
                        Sensitivity = Sen_final, Specificity = Spe_final)
Final_Results
```

\newpage

# 4.Conclusions

Binary classification is a frequent problem faced in many disciplines among science and real life. Supervised learning whenever enough historical data is collected offers a wide range of different algorithms to help us dealing with these problems and take better decisions.

I have tested logistic regression, k nearest neighbors, decision trees and random forest for a binary classification within a non uniformly distributed dataset where prevalence is indeed a challenging issue.

Despite the fact of prevalence, decision tree built allowed to obtain good accuracy and sensitivity and reasonable balanced accuracy and almost acceptable specificity. Apart from being significantly comprehensible, as shown in previous section, being able to classify in advance the results of your campaign allows the stakeholder to discriminate which sector of population focus on to get more revenues. 

Nevertheless, some more optimization could have been addressed specially on random forest because results on random forest have been far from expected ones a priori. Some extra analysis could also have been performed, specially due to imbalance nature of source dataset. Hence, building several different new samples from original dataset with more percentage of cases of non dominant segment of population to see if specificity could improve more is a possible future work. Understand the behavior of ROC curves or Gini index could be another interesting idea to progress with.

Finnaly, testing support vector machines and neuronal networks and comparing them with these results to confirm [@Moro] analysis are also another future possible ideas for future work.

\newpage

# 5.Bibliography
